{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#原始的main.py的代码\" data-toc-modified-id=\"原始的main.py的代码-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>原始的main.py的代码</a></span></li><li><span><a href=\"#make_train_data()——数据读取、封装、分发\" data-toc-modified-id=\"make_train_data()——数据读取、封装、分发-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>make_train_data()——数据读取、封装、分发</a></span><ul class=\"toc-item\"><li><span><a href=\"#handler()\" data-toc-modified-id=\"handler()-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>handler()</a></span><ul class=\"toc-item\"><li><span><a href=\"#打印调用CPU的数量\" data-toc-modified-id=\"打印调用CPU的数量-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>打印调用CPU的数量</a></span></li><li><span><a href=\"#raw_data-=-read_all_raw_data()——所有数据打包成类\" data-toc-modified-id=\"raw_data-=-read_all_raw_data()——所有数据打包成类-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>raw_data = read_all_raw_data()——所有数据打包成类</a></span><ul class=\"toc-item\"><li><span><a href=\"#DataHolder()\" data-toc-modified-id=\"DataHolder()-2.1.2.1\"><span class=\"toc-item-num\">2.1.2.1&nbsp;&nbsp;</span>DataHolder()</a></span><ul class=\"toc-item\"><li><span><a href=\"#read_raw_data()\" data-toc-modified-id=\"read_raw_data()-2.1.2.1.1\"><span class=\"toc-item-num\">2.1.2.1.1&nbsp;&nbsp;</span>read_raw_data()</a></span></li></ul></li></ul></li><li><span><a href=\"#分发数据以形成test_users、dataset、train_dataset、y_answer\" data-toc-modified-id=\"分发数据以形成test_users、dataset、train_dataset、y_answer-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>分发数据以形成test_users、dataset、train_dataset、y_answer</a></span></li><li><span><a href=\"#recall_results-=-calc_and_recall(dataset,-train_dataset,-test_users,-articles_dic,-cpu_cores,-offline,-y_answer)\" data-toc-modified-id=\"recall_results-=-calc_and_recall(dataset,-train_dataset,-test_users,-articles_dic,-cpu_cores,-offline,-y_answer)-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>recall_results = calc_and_recall(dataset, train_dataset, test_users, articles_dic, cpu_cores, offline, y_answer)</a></span><ul class=\"toc-item\"><li><span><a href=\"#sims-=-_calc_sim(dataset,-articles_dic,-cpu_cores,-offline)\" data-toc-modified-id=\"sims-=-_calc_sim(dataset,-articles_dic,-cpu_cores,-offline)-2.1.4.1\"><span class=\"toc-item-num\">2.1.4.1&nbsp;&nbsp;</span>sims = _calc_sim(dataset, articles_dic, cpu_cores, offline)</a></span><ul class=\"toc-item\"><li><span><a href=\"#i2i_30k_sim()\" data-toc-modified-id=\"i2i_30k_sim()-2.1.4.1.1\"><span class=\"toc-item-num\">2.1.4.1.1&nbsp;&nbsp;</span>i2i_30k_sim()</a></span><ul class=\"toc-item\"><li><span><a href=\"#_i2i_30k_sim_core()\" data-toc-modified-id=\"_i2i_30k_sim_core()-2.1.4.1.1.1\"><span class=\"toc-item-num\">2.1.4.1.1.1&nbsp;&nbsp;</span>_i2i_30k_sim_core()</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#create_train_data()\" data-toc-modified-id=\"create_train_data()-2.1.5\"><span class=\"toc-item-num\">2.1.5&nbsp;&nbsp;</span>create_train_data()</a></span><ul class=\"toc-item\"><li><span><a href=\"#ds-=-neg_sampling(ds)\" data-toc-modified-id=\"ds-=-neg_sampling(ds)-2.1.5.1\"><span class=\"toc-item-num\">2.1.5.1&nbsp;&nbsp;</span>ds = neg_sampling(ds)</a></span></li><li><span><a href=\"#get_user_features()\" data-toc-modified-id=\"get_user_features()-2.1.5.2\"><span class=\"toc-item-num\">2.1.5.2&nbsp;&nbsp;</span>get_user_features()</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#test()\" data-toc-modified-id=\"test()-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>test()</a></span><ul class=\"toc-item\"><li><span><a href=\"#prepare_dataset()\" data-toc-modified-id=\"prepare_dataset()-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>prepare_dataset()</a></span></li><li><span><a href=\"#make_recommend_dict()\" data-toc-modified-id=\"make_recommend_dict()-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>make_recommend_dict()</a></span></li><li><span><a href=\"#calc_mrr_and_hit()\" data-toc-modified-id=\"calc_mrr_and_hit()-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>calc_mrr_and_hit()</a></span></li></ul></li><li><span><a href=\"#make_test_data()\" data-toc-modified-id=\"make_test_data()-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>make_test_data()</a></span></li><li><span><a href=\"#run()\" data-toc-modified-id=\"run()-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>run()</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原始的main.py的代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后面是对这个代码的逐句分析，以及所有引用的包、其他.py文件引用的解释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from data_holder import DataHolder\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "from const import RAW_DATA_FOLDER, OUTPUT_FOLDER, CACHE_FOLDER\n",
    "from recaller import calc_and_recall\n",
    "from csv_handler import create_train_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def read_raw_data(filename, cb=None):\n",
    "    data = pd.read_csv(RAW_DATA_FOLDER + filename)\n",
    "    return cb(data) if cb is not None else data\n",
    "\n",
    "# def read_all_raw_data(filenames=['articles.csv', 'train_click_log.csv', 'testB_click_log_Test_B.csv', 'testA_click_log.csv']):\n",
    "#     return DataHolder(*[read_raw_data(filename) for filename in filenames])\n",
    "# 我确实没找到testB_click_log_Test_B.csv，所以我删掉了他，然后运行代码\n",
    "def read_all_raw_data(filenames=['articles.csv', 'train_click_log.csv', 'testA_click_log.csv']):\n",
    "    return DataHolder(*[read_raw_data(filename) for filename in filenames])\n",
    "\n",
    "\n",
    "def calc_mrr_and_hit(recommend_dict, y, k=5):\n",
    "    #assert len(recommend_dict) == len(y)\n",
    "    sum_mrr = 0.0\n",
    "    sum_hit = 0.0\n",
    "    sum_hit_detail = np.repeat(0.0, 5)\n",
    "    user_cnt = len(recommend_dict.keys())\n",
    "\n",
    "    for user_id, recommend_items in recommend_dict.items():\n",
    "        answer = y[user_id] if user_id in y else -1\n",
    "        if (answer in recommend_items) and (recommend_items.index(answer) < k):\n",
    "            sum_hit += 1\n",
    "            sum_mrr += 1 / (recommend_items.index(answer) + 1)\n",
    "            sum_hit_detail[recommend_items.index(answer)] += 1\n",
    "\n",
    "    return (sum_mrr / user_cnt), (sum_hit / user_cnt), (sum_hit_detail / user_cnt)\n",
    "\n",
    "def create_submission(recommend_dict):\n",
    "    _data = [{'user_id': user_id,\n",
    "        'article_1': art_id_list[0],\n",
    "        'article_2': art_id_list[1],\n",
    "        'article_3': art_id_list[2],\n",
    "        'article_4': art_id_list[3],\n",
    "        'article_5': art_id_list[4]} for user_id, art_id_list in tqdm(recommend_dict.items())]\n",
    "    _t = pd.DataFrame(_data)\n",
    "    _t.sort_values('user_id', inplace=True)\n",
    "    _t.to_csv(OUTPUT_FOLDER + 'result.csv', index=False)\n",
    "\n",
    "def handler(offline=True):\n",
    "    cpu_cores = mp.cpu_count()\n",
    "    print('使用CPU核心数: {}'.format(cpu_cores))\n",
    "    print('开始{}数据验证处理'.format('线下' if offline else '线上'))\n",
    "    raw_data = read_all_raw_data()\n",
    "    test_users = raw_data.get_test_users(offline)\n",
    "\n",
    "    _user_id_list = list(test_users.keys())\n",
    "    user_id_min = np.min(_user_id_list)\n",
    "    user_id_max = np.max(_user_id_list)\n",
    "    print('获得{}用户集合{}件 [{} ~ {}]'.format('验证' if offline else '测试', len(test_users), user_id_min, user_id_max))\n",
    "\n",
    "    dataset = raw_data.get_item_dt_groupby_user()\n",
    "\n",
    "    if offline:\n",
    "        train_dataset, y_answer = raw_data.get_train_dataset_and_answers(test_users)\n",
    "    else:\n",
    "        train_dataset = raw_data.get_train_dataset_for_online(test_users)\n",
    "        y_answer = None\n",
    "\n",
    "    print('训练数据({}件)'.format(np.sum([len(ts_list) for user_id, ts_list in train_dataset.items()])))\n",
    "\n",
    "    articles_dic = dict(list(raw_data.get_articles().apply(lambda x: (x['article_id'], dict(x)), axis=1)))\n",
    "    print('获得文章字典({}件)'.format(len(articles_dic.keys())))\n",
    "\n",
    "    recall_results = calc_and_recall(dataset, train_dataset, test_users, articles_dic, cpu_cores, offline, y_answer)\n",
    "    create_train_data(raw_data, train_dataset, test_users, articles_dic, recall_results, offline, y_answer)\n",
    "    print(\"(<finished>handler\")\n",
    "\n",
    "def make_train_data():\n",
    "    # Part_【1.1】_make_train_data\n",
    "    handler()\n",
    "    print(\"(<finished>Part_【1.1】_make_train_data_\")\n",
    "\n",
    "def make_test_data():\n",
    "    handler(False)\n",
    "    # Part_【2】make_test_data\n",
    "\n",
    "def prepare_dataset(df):\n",
    "    agg_column = [column for column in df.columns if column != 'user_id'][0]\n",
    "    df.sort_values('user_id', inplace=True)\n",
    "    grp_info = df.groupby('user_id', as_index=False).count()[agg_column].values\n",
    "    y = df['answer'] if 'answer' in df.columns else None\n",
    "    return df.drop(columns=['answer']) if 'answer' in df.columns else df, grp_info, y\n",
    "\n",
    "def make_recommend_dict(X_val, y_pred):\n",
    "    X_val['pred'] = y_pred\n",
    "    _t = X_val.groupby('user_id')\\\n",
    "        .apply(lambda x: list(x.sort_values('pred', ascending=False)['article_id'].head(5)))\\\n",
    "        .reset_index()\\\n",
    "        .rename(columns={0: 'item_list'})\n",
    "\n",
    "    recommend_dict = dict(zip(_t['user_id'], _t['item_list']))    \n",
    "    return recommend_dict\n",
    "\n",
    "def test():\n",
    "    df_train = pd.read_csv(CACHE_FOLDER + 'train.csv')\n",
    "\n",
    "    # clf = lgb.LGBMRanker(random_state=777, n_estimators=1000)\n",
    "    # 调低 n_estimators数量\n",
    "    clf = lgb.LGBMRanker(random_state=777, n_estimators=100)\n",
    "\n",
    "    users = df_train['user_id'].unique()\n",
    "    train_users, _test_users = train_test_split(users, test_size=0.2, random_state=98)\n",
    "    test_users, val_users = train_test_split(_test_users, test_size=0.5, random_state=38)\n",
    "    df_new_train = df_train.merge(pd.DataFrame(train_users, columns=['user_id']))\n",
    "    df_test = df_train.merge(pd.DataFrame(test_users, columns=['user_id']))\n",
    "    df_val = df_train.merge(pd.DataFrame(val_users, columns=['user_id']))\n",
    "\n",
    "    X_train, X_grp_train, y_train = prepare_dataset(df_new_train)\n",
    "    X_test, X_grp_test, y_test = prepare_dataset(df_test)\n",
    "    X_val, X_grp_val, _ = prepare_dataset(df_val)\n",
    "\n",
    "    def handle_columns(X):\n",
    "        return X.drop(columns=['user_id', 'article_id'])\n",
    "\n",
    "    _X_train = handle_columns(X_train)\n",
    "\n",
    "    clf.fit(_X_train, y_train, group=X_grp_train, eval_set=[(handle_columns(X_test), y_test)], eval_group=[X_grp_test], eval_at=[1, 2, 3, 4, 5], eval_metric=['ndcg', ], early_stopping_rounds=50, verbose=False)\n",
    "    print('Best iteration: {}'.format(clf.best_iteration_))\n",
    "\n",
    "\n",
    "    for X, X_grp, df, title in [(X_test, X_grp_test, df_test, 'Test Set'), (X_val, X_grp_val, df_val, 'Validation Set')]:\n",
    "        print('[{}]'.format(title))\n",
    "        y_pred = clf.predict(handle_columns(X), group=X_grp, num_iteration=clf.best_iteration_)\n",
    "        recommend_dict = make_recommend_dict(X, y_pred)\n",
    "        answers = dict(df.loc[df['answer'] == 1, ['user_id', 'article_id']].values)\n",
    "        mrr, hit, details = calc_mrr_and_hit(recommend_dict, answers)\n",
    "        print('MRR: {} / HIT: {}'.format(mrr, hit))\n",
    "        print(' / '.join(['%.2f' % detail for detail in details]))\n",
    "\n",
    "    for column, score in sorted(zip(_X_train.columns, clf.feature_importances_), key=lambda x: x[1], reverse=True):\n",
    "        print('{}: {}'.format(column, score))\n",
    "    print(\" Part_【1.2】make_test_data\")\n",
    "\n",
    "def run():\n",
    "    df_train = pd.read_csv(CACHE_FOLDER + 'train.csv')\n",
    "    df_test = pd.read_csv(CACHE_FOLDER + 'test.csv')\n",
    "\n",
    "    # clf = lgb.LGBMRanker(random_state=777, n_estimators=1000)\n",
    "    # !!!!!!!!!建议你将n_estimators变小一些，这样运算速度快!!!!!!!!!!!\n",
    "    # 调低 n_estimators数量\n",
    "    clf = lgb.LGBMRanker(random_state=777, n_estimators=100)\n",
    "\n",
    "    users = df_train['user_id'].unique()\n",
    "    train_users, eval_users = train_test_split(users, test_size=0.2, random_state=77)\n",
    "    df_new_train = df_train.merge(pd.DataFrame(train_users, columns=['user_id']))\n",
    "    df_eval = df_train.merge(pd.DataFrame(eval_users, columns=['user_id']))\n",
    "\n",
    "    X_train, X_grp_train, y_train = prepare_dataset(df_new_train)\n",
    "    X_eval, X_grp_eval, y_eval = prepare_dataset(df_eval)\n",
    "    X_test, X_grp_test, _ = prepare_dataset(df_test)\n",
    "\n",
    "    def handle_columns(X):\n",
    "        return X.drop(columns=['user_id', 'article_id'])\n",
    "\n",
    "    _X_train = handle_columns(X_train)\n",
    "\n",
    "    clf.fit(_X_train, y_train, group=X_grp_train, eval_set=[(handle_columns(X_eval), y_eval)], eval_group=[X_grp_eval], eval_at=[1, 2, 3, 4, 5], eval_metric=['ndcg', ], early_stopping_rounds=50, verbose=False)\n",
    "    print('Best iteration: {}'.format(clf.best_iteration_))\n",
    "    y_pred = clf.predict(handle_columns(X_test), group=X_grp_test, num_iteration=clf.best_iteration_)\n",
    "    \n",
    "    for column, score in sorted(zip(_X_train.columns, clf.feature_importances_), key=lambda x: x[1], reverse=True):\n",
    "        print('{}: {}'.format(column, score))\n",
    "\n",
    "    recommend_dict = make_recommend_dict(X_test, y_pred)\n",
    "\n",
    "    create_submission(recommend_dict)\n",
    "    print(\"<finished>Part_【3】run\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"<Start># Part_【1】_make_train_data\")\n",
    "    make_train_data()\n",
    "    print(\"<Start># Part_【1.2】make_test_data\")\n",
    "    test()\n",
    "    print(\"<Start># Part_【2】make_test_data\")\n",
    "    make_test_data()\n",
    "    print(\"<Start># Part_【3】run\")\n",
    "    run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make_train_data()——数据读取、封装、分发"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"<Start># Part_【1】_make_train_data\");print()\n",
    "make_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_data():\n",
    "    # Part_【1.1】_make_train_data\n",
    "    handler()\n",
    "    print(\"(<finished>Part_【1.1】_make_train_data_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## handler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(offline=True):\n",
    "    \n",
    "#     2.1.1  打印调用CPU的数量\n",
    "    # 数一下一共多少个cpu被使用了\n",
    "    cpu_cores = mp.cpu_count()\n",
    "    print('使用CPU核心数: {}'.format(cpu_cores))\n",
    "    print('开始{}数据验证处理'.format('线下' if offline else '线上'))\n",
    "    \n",
    "    # 2.1.1  read_all_raw_data()\n",
    "    raw_data = read_all_raw_data()\n",
    "    test_users = raw_data.get_test_users(offline) # ？？？？这句话不懂了，没头没尾，为什raw_data有get_test_users的方法？\n",
    "    \n",
    "    _user_id_list = list(test_users.keys())\n",
    "    user_id_min = np.min(_user_id_list)\n",
    "    user_id_max = np.max(_user_id_list)\n",
    "    # 线下就说“验证”，线上就说“测试”，说明test_users的数量，id是从几到几\n",
    "    print('获得{}用户集合{}件 [{} ~ {}]'.format('验证' if offline else '测试', len(test_users), user_id_min, user_id_max))\n",
    "\n",
    "    dataset = raw_data.get_item_dt_groupby_user()\n",
    "    \n",
    "    # 将所有数据文件封装起来的raw_data里面的数据，分发给各个变量\n",
    "    if offline:\n",
    "        train_dataset, y_answer = raw_data.get_train_dataset_and_answers(test_users)\n",
    "    else:\n",
    "        train_dataset = raw_data.get_train_dataset_for_online(test_users)\n",
    "        y_answer = None\n",
    "\n",
    "    print('训练数据({}件)'.format(np.sum([len(ts_list) for user_id, ts_list in train_dataset.items()])))\n",
    "\n",
    "    articles_dic = dict(list(raw_data.get_articles().apply(lambda x: (x['article_id'], dict(x)), axis=1)))\n",
    "    print('获得文章字典({}件)'.format(len(articles_dic.keys())))\n",
    "\n",
    "    \n",
    "#     1.1.2 calc_and_recall\n",
    "    recall_results = calc_and_recall(dataset, train_dataset, test_users, articles_dic, cpu_cores, offline, y_answer)\n",
    "    \n",
    "    \n",
    "    create_train_data(raw_data, train_dataset, test_users, articles_dic, recall_results, offline, y_answer)\n",
    "    print(\"(<finished>handler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打印调用CPU的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 数一下一共多少个cpu被使用了\n",
    "    cpu_cores = mp.cpu_count()\n",
    "    print('使用CPU核心数: {}'.format(cpu_cores))\n",
    "    print('开始{}数据验证处理'.format('线下' if offline else '线上'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  raw_data = read_all_raw_data()——所有数据打包成类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'testB_click_log_Test_B.csv' 这个文件是我问王百万学长要的，网上没有提供\n",
    "# 你删除掉这个文件并不影响程序的运行\n",
    "# 你加上了反而报错——key error,应该是数据的维度不对————暂时删掉了，后面去解决这个错误\n",
    "\n",
    "# def read_all_raw_data(filenames=['articles.csv', 'train_click_log.csv', 'testB_click_log_Test_B.csv', 'testA_click_log.csv']):\n",
    "#     return DataHolder(*[read_raw_data(filename) for filename in filenames])\n",
    "\n",
    "def read_all_raw_data(filenames=['articles.csv', 'train_click_log.csv', 'testA_click_log.csv']):\n",
    "    return DataHolder(*[read_raw_data(filename) for filename in filenames])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataHolder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里注意这个DatsHolder是下面这句话引入的。这个类里面就是有get_test_users（）和get_item_dt_groupby_user（），get_train_dataset_and_answers（）的方法，可以生成所需的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataHolder这个类的定义信息如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHolder:\n",
    "    # 我 实际调用向内传入['articles.csv', 'train_click_log.csv', 'testA_click_log.csv']\n",
    "        # articles代表'articles.csv'：文章的基本信息（文章的id、类别、创建时间、字数）\n",
    "\n",
    "        # train_click_log代表\"train_click_log.csv\":\n",
    "        # test_click_log代表\"testA_click_log.csv\"\n",
    "        # trainB_click_log代表\"testB_click_log_Test_B.csv\"(可以不填，不填就是None)\n",
    "            # 上面这三组数据，维度都是一样的，每行代表一个用户的一次点击\n",
    "            # 每一列分别是，用户的id,点击的文章的代号id,点击的时间，以及每次点击相关的数据（文章的类别，什么平台点击的，什么设备点击，等等很多列）\n",
    "    def __init__(self, articles, train_click_log, test_click_log, trainB_click_log=None):\n",
    "        # 【1】将输入的数据分发出去\n",
    "        self.articles = articles\n",
    "        self.train_click_log = train_click_log\n",
    "        self.test_click_log = test_click_log\n",
    "        self.trainB_click_log = trainB_click_log\n",
    "            # all_click_log表示的意思是，\n",
    "                # 如果trainB_click_log非空，就按照（self.train_click_log；self.trainB_click_log；self.test_click_log）3个做一个数据集合并\n",
    "                # 如果 没有trainB_click_log，就只合并（self.train_click_log；self.test_click_log）2个\n",
    "        self.all_click_log = self.train_click_log.append(self.trainB_click_log).append(self.test_click_log) if self.trainB_click_log is not None else self.train_click_log.append(self.test_click_log)\n",
    "                # 将合并的数据集里面，user_id，文章_id,点击时间有重复的删除————删除一些冗余的信息\n",
    "        self.all_click_log.drop_duplicates(['user_id', 'click_article_id', 'click_timestamp'], inplace=True)\n",
    "\n",
    "        # 将读取的这些数据的基本信息打印出来\n",
    "        print('从train_click_log读取{}件(UserId=[{},{}])'.format(len(self.train_click_log), self.train_click_log['user_id'].min(), self.train_click_log['user_id'].max()))\n",
    "        print('从test_click_log读取{}件(UserId=[{},{}])'.format(len(self.test_click_log), self.test_click_log['user_id'].min(), self.test_click_log['user_id'].max()))\n",
    "        if self.trainB_click_log is not None:\n",
    "            print('从trainB_click_log读取{}件(UserId=[{},{}])'.format(len(self.trainB_click_log), self.trainB_click_log['user_id'].min(), self.trainB_click_log['user_id'].max()))\n",
    "        print('使用训练集all_click_log共{}件(UserId=[{},{}])'.format(len(self.all_click_log), self.all_click_log['user_id'].min(), self.all_click_log['user_id'].max()))\n",
    "\n",
    "\n",
    "        # 【2.1】 将你合并好的所有数据DataFrame，存储成dataset.pkl文件，转为字典格式命名为self.dataset\n",
    "            # 存储成.pkl文件以后，你第二次不必再做一次数据合并和转dict了\n",
    "            # 字典应该是运算速度比dataframe更快，所以选择这种数据类型\n",
    "        filename = 'dataset.pkl'\n",
    "        if isfile(CACHE_FOLDER + filename):\n",
    "            print('直接从文件{}中读取dataset'.format(filename))\n",
    "                # \"rb\"就是reading模式，\"wb\"是write模式\n",
    "            self.dataset = pickle.load(open(CACHE_FOLDER + filename, 'rb'))\n",
    "        else:\n",
    "            start_time = time.time()\n",
    "            #\n",
    "            _t = self.all_click_log.sort_values('click_timestamp').groupby('user_id')\\\n",
    "                .apply(lambda x: list(zip(x['click_article_id'], x['click_timestamp'])))\\\n",
    "                .reset_index()\\\n",
    "                .rename(columns={0: 'item_dt_list'})\n",
    "            self.dataset = dict(zip(_t['user_id'], _t['item_dt_list']))\n",
    "            print('dataset对象完毕({}秒)'.format('%.2f' % (time.time() - start_time)))\n",
    "            print('保存dataset至文件{}中'.format(filename))\n",
    "            pickle.dump(self.dataset, open(CACHE_FOLDER + filename, 'wb'))\n",
    "\n",
    "        # 【2.2】 将你合并好的所有数据DataFrame，存储成train_users_dic.pkl文件，转为字典格式命名为self.train_users_dic\n",
    "            # 字典内容为(user_id, timestamp)，字典用于供训练用\n",
    "        filename = 'train_users_dic.pkl'\n",
    "        if isfile(CACHE_FOLDER + filename):\n",
    "            print('直接从文件{}中读取train_users_dic'.format(filename))\n",
    "            self.train_users_dic = pickle.load(open(CACHE_FOLDER + filename, 'rb'))\n",
    "        else:\n",
    "            start_time = time.time()\n",
    "            self.train_users_dic = {}\n",
    "                # 这里这个tqdm，代表的意思就是进度条那个东西\n",
    "            for user_id, items in tqdm(self.dataset.items()):\n",
    "                ts_list = pd.Series([item[1] for item in items])\n",
    "                    # 向train_users_dic里面添加key为user_id，value为后面的东西\n",
    "                self.train_users_dic[user_id] = list(ts_list.loc[ts_list.shift(-1) - ts_list == 30000])\n",
    "            print('train_users_dic对象完毕({}秒)'.format('%.2f' % (time.time() - start_time)))\n",
    "            print('保存train_users_dic至文件{}中'.format(filename))\n",
    "            pickle.dump(self.train_users_dic, open(CACHE_FOLDER + filename, 'wb'))\n",
    "\n",
    "\n",
    "    # 【3】写好调用接口，后面分发数据，就用下面的\n",
    "    def get_articles(self):\n",
    "        return self.articles\n",
    "    def get_train_click_log(self):\n",
    "        return self.train_click_log\n",
    "    def get_test_click_log(self):\n",
    "        return self.test_click_log\n",
    "    def get_all_click_log(self):\n",
    "        return self.all_click_log\n",
    "\n",
    "    def get_user_list(self):\n",
    "        # 返回用户的id\n",
    "        return self.train_click_log['user_id'].unique()\n",
    "    def get_item_dt_groupby_user(self):\n",
    "        return self.dataset\n",
    "\n",
    "\n",
    "    def users_df2dic(self, df_users):\n",
    "        _t = df_users.sort_values('click_timestamp').groupby('user_id')\\\n",
    "            .apply(lambda x: set(x['click_timestamp']))\\\n",
    "            .reset_index()\\\n",
    "            .rename(columns={0: 'ts_set'})\n",
    "        return dict(zip(_t['user_id'], _t['ts_set']))\n",
    "\n",
    "    # 生成测试所需的数据\n",
    "        # 详细解释：应该是用来将过去的数据（self.train_users_dic），进行格式处理，然后随机取样\n",
    "    def get_test_users(self, offline, samples=100000):\n",
    "        if offline:\n",
    "            # 一维数组化\n",
    "            users = []\n",
    "            for user_id, ts_list in self.train_users_dic.items():\n",
    "                # for ts in ts_list:\n",
    "                #     users.append((user_id, ts))\n",
    "                if len(ts_list) > 0:\n",
    "                    users.append((user_id, ts_list[-1]))\n",
    "            np.random.seed(42)\n",
    "            idx_list = np.random.choice(len(users), samples, replace=False)\n",
    "            selected_users = [users[idx] for idx in idx_list]\n",
    "            # 字典化\n",
    "            return self.users_df2dic(pd.DataFrame(selected_users, columns=['user_id', 'click_timestamp']))\n",
    "        else:\n",
    "            return self.users_df2dic(self.test_click_log.groupby('user_id').max('click_timestamp').reset_index()[['user_id', 'click_timestamp']])\n",
    "\n",
    "    def take_last(self, items, last=1):\n",
    "        if len(items) <= last:\n",
    "            return items.copy(), items[0]\n",
    "        else:\n",
    "            return items[:-last], items[-last]\n",
    "\n",
    "    # 用来将dataset和预测值（y_answer)的混合字典，拆分成两个字典\n",
    "    def get_train_dataset_and_answers(self, test_users):\n",
    "        start_time = time.time()\n",
    "        train_dataset = {}\n",
    "        y_answer = {}\n",
    "\n",
    "            # tqdm，代表进度条\n",
    "        for user_id, ts_set in tqdm(test_users.items()):\n",
    "            items = self.dataset[user_id]\n",
    "            for last_clicked_timestamp in ts_set:\n",
    "                idx = [item[1] for item in items].index(last_clicked_timestamp)\n",
    "                train_dataset.setdefault(user_id, {})\n",
    "                train_dataset[user_id][last_clicked_timestamp] = items[0:idx+1]\n",
    "                y_answer.setdefault(user_id, {})\n",
    "                y_answer[user_id][last_clicked_timestamp] = items[idx+1][0]\n",
    "        print('训练集和答案分割完毕({}秒)'.format('%.2f' % (time.time() - start_time)))\n",
    "        return train_dataset, y_answer\n",
    "\n",
    "    def get_train_dataset_for_online(self, test_users):\n",
    "        start_time = time.time()\n",
    "        train_dataset = {}\n",
    "\n",
    "        for user_id, ts_set in tqdm(test_users.items()):\n",
    "            items = self.dataset[user_id]\n",
    "            for last_clicked_timestamp in ts_set:\n",
    "                train_dataset.setdefault(user_id, {})\n",
    "                train_dataset[user_id][last_clicked_timestamp] = items\n",
    "        print('测试集制作完毕({}秒)'.format('%.2f' % (time.time() - start_time)))\n",
    "        return train_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### read_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_data(filename, cb=None):\n",
    "    data = pd.read_csv(RAW_DATA_FOLDER + filename)\n",
    "    return cb(data) if cb is not None else data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from const import RAW_DATA_FOLDER, OUTPUT_FOLDER, CACHE_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# const.py这个文件中这样写\n",
    "# # RAW_DATA_FOLDER = '../tcdata/'\n",
    "# # OUTPUT_FOLDER = '../prediction_result/'\n",
    "# # CACHE_FOLDER = '../user_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面是读取本文件上一一层文件夹下的tcdata内的这些文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是把['articles.csv', 'train_click_log.csv', 'testB_click_log_Test_B.csv', 'testA_click_log.csv']这些文件夹逐一读取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分发数据以形成test_users、dataset、train_dataset、y_answer\n",
    "调用类的实例（raw_data）方法，以实现数据分发\n",
    "\n",
    "test_users = raw_data.get_test_users(offline)\n",
    "\n",
    "dataset = raw_data.get_item_dt_groupby_user()——字典，用户id:[用户所有的点击数据]（待定？？）\n",
    "\n",
    "train_dataset, y_answer = raw_data.get_train_dataset_and_answers(test_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    test_users = raw_data.get_test_users(offline)\n",
    "    _user_id_list = list(test_users.keys())\n",
    "    user_id_min = np.min(_user_id_list)\n",
    "    user_id_max = np.max(_user_id_list)\n",
    "    print('获得{}用户集合{}件 [{} ~ {}]'.format('验证' if offline else '测试', len(test_users), user_id_min, user_id_max))\n",
    "    dataset = raw_data.get_item_dt_groupby_user()\n",
    "    if offline:\n",
    "        train_dataset, y_answer = raw_data.get_train_dataset_and_answers(test_users)\n",
    "    else:\n",
    "        train_dataset = raw_data.get_train_dataset_for_online(test_users)\n",
    "        y_answer = None\n",
    "    print('训练数据({}件)'.format(np.sum([len(ts_list) for user_id, ts_list in train_dataset.items()])))\n",
    "    articles_dic = dict(list(raw_data.get_articles().apply(lambda x: (x['article_id'], dict(x)), axis=1)))\n",
    "    print('获得文章字典({}件)'.format(len(articles_dic.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recall_results = calc_and_recall(dataset, train_dataset, test_users, articles_dic, cpu_cores, offline, y_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_results = calc_and_recall(dataset, train_dataset, test_users, articles_dic, cpu_cores, offline, y_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc_and_recall 是通过import进来的，来自recaller.py\n",
    "\n",
    "# from recaller import calc_and_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calc_and_recall的输入变量都代表什么意思呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_and_recall(dataset, train_dataset, test_users, articles_dic, cpu_cores, offline, answers=None):\n",
    "#     # **这些输入变量分别代表什么实际的意思？**\n",
    "#     # dataset——字典，用户id: [用户所有的点击数据]（待定？？）\n",
    "#         # （main.py->data_holder.py)dataset = raw_data.get_item_dt_groupby_user()\n",
    "#     # train_dataset_，应该是代表各种各样的用户信息\n",
    "#         # （main.py->data_holder.py)train_dataset, y_answer = raw_data.get_train_dataset_and_answers(test_users)\n",
    "#     # test_users——字典，从self.train_users_dic中随机抽取sampels个用户的信息（待定？？）\n",
    "#         # （main.py->data_holder.py) test_users = raw_data.get_test_users(offline)\n",
    "#     # articles_dic——'articles.csv'的内容-：文章的基本信息（文章的id、类别、创建时间、字数）\n",
    "#         # 文章id:文章所有的信息（待定？？）\n",
    "#         # （main.py->data_holder.py) articles_dic = dict(list(raw_data.get_articles().apply(lambda x: (x['article_id'], dict(x)), axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sims = _calc_sim(dataset, articles_dic, cpu_cores, offline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _calc_sim 这个函数就定义在recallr.py这个文件内"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_calc_sim中的输入的变量代表什么意思？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _calc_sim(dataset, articles_dic, cpu_cores, offline):\n",
    "#     # ***输入变量代表什么意思？***\n",
    "#     # dataset——字典，用户id: [用户所有的点击数据]（待定？？）\n",
    "#         # （main.py->data_holder.py)dataset = raw_data.get_item_dt_groupby_user()\n",
    "#     # articles_dic——'articles.csv'的内容-：文章的基本信息（文章的id、类别、创建时间、字数）\n",
    "#         # 文章id:文章所有的信息（待定？？）\n",
    "#         # （main.py->data_holder.py) articles_dic = dict(list(raw_data.get_articles().apply(lambda x: (x['article_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i2i_30k_sim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用这个函数的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # 【2.1.4.1.1】  i2i_30k_sim\n",
    "#     # 计算各种相似度\n",
    "#     num = len([i2i_30k_sim])\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     print('召回前的计算处理开始({}件)'.format(num))\n",
    "\n",
    "#     sims = {}\n",
    "#     sims['i2i_30k_sim'] = i2i_30k_sim(dataset, cpu_cores, offline)\n",
    "\n",
    "#     print('召回前的计算处理结束({}秒)'.format('%.2f' % (time.time() - start_time)))\n",
    "\n",
    "#     return sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数的完整的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def i2i_30k_sim(dataset, n_cpu, offline, max_related=50):\n",
    "#     # ***传入的变量代表的意思是什么？***\n",
    "#     # dataset——字典，用户id: [用户所有的点击数据]（待定？？）\n",
    "#         # （main.py->data_holder.py)dataset = raw_data.get_item_dt_groupby_user()\n",
    "\n",
    "#     # 填充进去的文字是在大括号那个位置\n",
    "#     filename = 'i2i_30k_sim_{}.pkl'.format('offline' if offline else 'online')\n",
    "#     # 如果这个地址（'../user_data/'）有上面那个字符串那个样子的pkl模型文件，就直接返回这个模型的信息，\n",
    "#     #   之前计算过，就不必重复计算了，读取之前算好的就行了\n",
    "#     if isfile(CACHE_FOLDER + filename):\n",
    "#         print('直接从文件{}中读取计算好的i2i_30k相似度'.format(filename))\n",
    "#         return pickle.load(open(CACHE_FOLDER + filename, 'rb'))\n",
    "\n",
    "#     # 计算相似度\n",
    "#     start_time = time.time()\n",
    "#     print('开始计算i2i_30k相似度')\n",
    "#     i2i_sim_3k = {}\n",
    "#         # 分配到每一个cpu上有多少个用户的点击数据\n",
    "#     n_block = (len(dataset.keys()) - 1) // n_cpu + 1\n",
    "#     keys = list(dataset.keys())\n",
    "#         # 这个可以让你并行使用多个cpu核\n",
    "#     pool = mp.Pool(processes=n_cpu)\n",
    "#         # 进程任务拆分。给每个cpu指定了任务。（不太懂？？）\n",
    "#     results = [pool.apply_async(_i2i_30k_sim_core, args=(i, keys[i * n_block:(i + 1) * n_block], dataset)) for i in range(0, n_cpu)]\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "\n",
    "#     for result in results:\n",
    "#         _i2i_sim_3k = result.get()\n",
    "\n",
    "#         for art_id, related_art_id_dic in _i2i_sim_3k.items():\n",
    "#             i2i_sim_3k.setdefault(art_id, {})\n",
    "#             for related_art_id, value in related_art_id_dic.items():\n",
    "#                 i2i_sim_3k[art_id].setdefault(related_art_id, 0)\n",
    "#                 i2i_sim_3k[art_id][related_art_id] += value\n",
    "\n",
    "#     print('逆序排序')\n",
    "#     for art_id, related_arts in tqdm(i2i_sim_3k.items()):\n",
    "#         sorted_and_topK = sorted(related_arts.items(), key=lambda x: x[1], reverse=True)\n",
    "#         i2i_sim_3k[art_id] = {\n",
    "#             'sorted_keys': [art_id for art_id, _ in sorted_and_topK],\n",
    "#             'related_arts': dict(sorted_and_topK)\n",
    "#         } \n",
    "\n",
    "#     print('i2i_30k相似度计算完毕({}秒)'.format('%.2f' % (time.time() - start_time)))\n",
    "#     print('保存i2i_30k相似度数据至文件{}中'.format(filename))\n",
    "#     pickle.dump(i2i_sim_3k, open(CACHE_FOLDER + filename, 'wb'))\n",
    "#     return i2i_sim_3k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体解释如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 填充进去的文字是在大括号那个位置\n",
    "#     filename = 'i2i_30k_sim_{}.pkl'.format('offline' if offline else 'online')\n",
    "#     # 如果这个地址（'../user_data/'）有上面那个字符串那个样子的pkl模型文件，就直接返回这个模型的信息，\n",
    "#     #   之前计算过，就不必重复计算了，读取之前算好的就行了\n",
    "#     if isfile(CACHE_FOLDER + filename):\n",
    "#         print('直接从文件{}中读取计算好的i2i_30k相似度'.format(filename))\n",
    "#         return pickle.load(open(CACHE_FOLDER + filename, 'rb'))\n",
    "\n",
    "#     # 计算相似度\n",
    "#     start_time = time.time()\n",
    "#     print('开始计算i2i_30k相似度')\n",
    "#     i2i_sim_3k = {}\n",
    "#         # 分配到每一个cpu上有多少个用户的点击数据\n",
    "#     n_block = (len(dataset.keys()) - 1) // n_cpu + 1\n",
    "#     keys = list(dataset.keys())\n",
    "#         # 这个可以让你并行使用多个cpu核\n",
    "#     pool = mp.Pool(processes=n_cpu)\n",
    "#         # 进程任务拆分。给每个cpu指定了任务。（不太懂？？）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### _i2i_30k_sim_core()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用这个函数的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [pool.apply_async(_i2i_30k_sim_core, args=(i, keys[i * n_block:(i + 1) * n_block], dataset)) for i in range(0, n_cpu)]\n",
    "    pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数定义的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _i2i_30k_sim_core(job_id, user_id_list, dataset):\n",
    "#     # ***传进去每个变量代表什么意思***\n",
    "#     # 一共传进去3个变量，i 、若干个keys、dataset\n",
    "#     # i：代表使用的是第几个cpu\n",
    "#     # 所有这些用户的id(keys)分配给每个cpu的id是哪些\n",
    "#     # dataset——字典，用户id: [用户所有的点击数据]（待定？？\n",
    "#     _item_counts_dic = {}\n",
    "#     _i2i_30k_sim = {}\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     # 从所有id的这个用户id里，把每个id逐一取出来\n",
    "#     for user_id in user_id_list:\n",
    "#         # 从dataset中将每个用户对应的的数据（device类型，所处位置，文章类型等等）提取出来\n",
    "#         item_dt = dataset[user_id]\n",
    "#         # 所有用户的数据拆分成两部分，抛弃第一部分，保留第二部分\n",
    "#         ts_list = pd.Series([ts for _, ts in item_dt])\n",
    "#         # 不太懂，应该就是提取了一部分吧，这里30000的原因是“训练集每个用户最后两次点击差值皆为30000”\n",
    "#         idx_list = [idx for idx, val in dict(ts_list - ts_list.shift(1) == 30000).items() if val]\n",
    "\n",
    "#         for idx in idx_list:\n",
    "#             i_art_id, _ = item_dt[idx]\n",
    "#             j_art_id, _ = item_dt[idx - 1]\n",
    "\n",
    "#             _i2i_30k_sim.setdefault(i_art_id, {})\n",
    "#             _i2i_30k_sim[i_art_id].setdefault(j_art_id, 0)\n",
    "#             _i2i_30k_sim[i_art_id][j_art_id] += 1\n",
    "\n",
    "#             _i2i_30k_sim.setdefault(j_art_id, {})\n",
    "#             _i2i_30k_sim[j_art_id].setdefault(i_art_id, 0)\n",
    "#             _i2i_30k_sim[j_art_id][i_art_id] += 1\n",
    "\n",
    "#     print('子任务[{}]: 完成i2i_30k相似度的计算。({}秒)'.format(job_id, '%.2f' % (time.time() - start_time)))\n",
    "\n",
    "#     return _i2i_30k_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下代码，太复杂，先不看，完全不懂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  pool.close()\n",
    "#     pool.join()\n",
    "\n",
    "#     for result in results:\n",
    "#         # mp.Pool(processes=n_cpu) 这个类有get方法\n",
    "#         _i2i_sim_3k = result.get()\n",
    "\n",
    "#         for art_id, related_art_id_dic in _i2i_sim_3k.items():\n",
    "#             i2i_sim_3k.setdefault(art_id, {})\n",
    "#             for related_art_id, value in related_art_id_dic.items():\n",
    "#                 i2i_sim_3k[art_id].setdefault(related_art_id, 0)\n",
    "#                 i2i_sim_3k[art_id][related_art_id] += value\n",
    "\n",
    "#     print('逆序排序')\n",
    "#     for art_id, related_arts in tqdm(i2i_sim_3k.items()):\n",
    "#         sorted_and_topK = sorted(related_arts.items(), key=lambda x: x[1], reverse=True)\n",
    "#         i2i_sim_3k[art_id] = {\n",
    "#             'sorted_keys': [art_id for art_id, _ in sorted_and_topK],\n",
    "#             'related_arts': dict(sorted_and_topK)\n",
    "#         } \n",
    "\n",
    "#     print('i2i_30k相似度计算完毕({}秒)'.format('%.2f' % (time.time() - start_time)))\n",
    "#     print('保存i2i_30k相似度数据至文件{}中'.format(filename))\n",
    "#     pickle.dump(i2i_sim_3k, open(CACHE_FOLDER + filename, 'wb'))\n",
    "#     return i2i_sim_3k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用这个函数的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_train_data(raw_data, train_dataset, test_users, articles_dic, recall_results, offline, y_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数（create_train_data）是从csv_handler.py中引入进来的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from csv_handler import create_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数的定义代码如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_train_data(raw_data, train_dataset, test_users, articles_dic, recall_results, offline, y_answer):\n",
    "#     start_time = time.time()\n",
    "#     keys_ds = []\n",
    "\n",
    "#     for user_id, ts_set in test_users.items():\n",
    "#         for last_clicked_timestamp in ts_set:\n",
    "#             items = np.concatenate([result[user_id][last_clicked_timestamp] for _, result in recall_results.items()])\n",
    "#             keys_ds.append(list(zip(np.repeat(user_id, len(items)), np.repeat(last_clicked_timestamp, len(items)), items)))\n",
    "\n",
    "#     ds = pd.DataFrame(np.concatenate(keys_ds), columns=['user_id', 'last_clicked_timestamp', 'article_id'], dtype=np.int64).drop_duplicates()\n",
    "\n",
    "#     if offline:\n",
    "#         answer_keys_ds = []\n",
    "#         # 拼接正确答案标签\n",
    "#         for user_id, ts_list in y_answer.items():\n",
    "#             for last_clicked_timestamp, art_id in ts_list.items():\n",
    "#                 answer_keys_ds.append((user_id, last_clicked_timestamp, art_id))\n",
    "\n",
    "#         answers = pd.DataFrame(answer_keys_ds, columns=['user_id', 'last_clicked_timestamp', 'article_id'], dtype=np.int64)\n",
    "#         # 将正确答案融合进数据集\n",
    "#         answers['answer'] = 1\n",
    "#         ds = ds.merge(answers, how='left').fillna({'answer': 0})\n",
    "#         ds['answer'] = ds['answer'].astype(np.int8)\n",
    "\n",
    "#         # 负采样\n",
    "#         ds = neg_sampling(ds)\n",
    "\n",
    "#     ds = ds.merge(raw_data.get_articles()).merge(get_user_features(raw_data, train_dataset, test_users, articles_dic))\n",
    "\n",
    "#     # 新特征\n",
    "#     ds['lag_period_last_article'] = ds['last_clicked_timestamp'] - ds['created_at_ts']\n",
    "#     ds['diff_words_last_article'] = ds['avg_words_count'] - ds['words_count']\n",
    "#     ds.to_csv(CACHE_FOLDER + '{}.csv'.format('train' if offline else 'test'), index=False)\n",
    "#     print('{}用的csv文件生成完毕({}秒, {}件)'.format('训练' if offline else '测试', '%.2f' % (time.time() - start_time), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体解释如下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> 实话说这段代码一句也没看懂，应该逐一运行把每个变量print出来，看看每个变量代表什么意思"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     start_time = time.time()\n",
    "#     keys_ds = []\n",
    "\n",
    "#     for user_id, ts_set in test_users.items():\n",
    "#         for last_clicked_timestamp in ts_set:\n",
    "#             items = np.concatenate([result[user_id][last_clicked_timestamp] for _, result in recall_results.items()])\n",
    "#             keys_ds.append(list(zip(np.repeat(user_id, len(items)), np.repeat(last_clicked_timestamp, len(items)), items)))\n",
    "\n",
    "#     ds = pd.DataFrame(np.concatenate(keys_ds), columns=['user_id', 'last_clicked_timestamp', 'article_id'], dtype=np.int64).drop_duplicates()\n",
    "\n",
    "#     if offline:\n",
    "#         answer_keys_ds = []\n",
    "#         # 拼接正确答案标签\n",
    "#         for user_id, ts_list in y_answer.items():\n",
    "#             for last_clicked_timestamp, art_id in ts_list.items():\n",
    "#                 answer_keys_ds.append((user_id, last_clicked_timestamp, art_id))\n",
    "\n",
    "#         answers = pd.DataFrame(answer_keys_ds, columns=['user_id', 'last_clicked_timestamp', 'article_id'], dtype=np.int64)\n",
    "#         # 将正确答案融合进数据集\n",
    "#         answers['answer'] = 1\n",
    "#         ds = ds.merge(answers, how='left').fillna({'answer': 0})\n",
    "#         ds['answer'] = ds['answer'].astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ds = neg_sampling(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用这个函数的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = neg_sampling(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数定义在csv_handler.py里面，原始代码如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def neg_sampling(ds, min=1, max=5):\n",
    "#     start_time = time.time()\n",
    "#     pos_ds = ds.loc[ds['answer'] == 1]\n",
    "#     neg_ds = ds.loc[ds['answer'] == 0]\n",
    "\n",
    "#     def _neg_sampling_func(x):\n",
    "#         n_sampling = len(x)\n",
    "#         n_sampling = min if n_sampling < min else (max if n_sampling > max else n_sampling)\n",
    "#         return x.sample(n=n_sampling, replace=False)\n",
    "\n",
    "#     neg_ds = pd.concat([\n",
    "#         neg_ds.groupby(['user_id', 'last_clicked_timestamp']).apply(_neg_sampling_func),\n",
    "#         neg_ds.groupby('article_id').apply(_neg_sampling_func),\n",
    "#         ]).drop_duplicates()\n",
    "\n",
    "#     ret = pd.concat([pos_ds, neg_ds]).reset_index(drop=True)\n",
    "#     print('负采样处理完毕({}秒, {}->{}件)'.format('%.2f' % (time.time() - start_time), len(ds), len(ret)))\n",
    "#     return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_user_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用这个函数的地方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = ds.merge(raw_data.get_articles()).merge(get_user_features(raw_data, train_dataset, test_users, articles_dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数的定义代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【问：】get_user_features()的输出是什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_user_features(raw_data, train_dataset, test_users, articles_dic):\n",
    "#     def calc_avg_words_count(items):\n",
    "#         return np.average([articles_dic[item[0]]['words_count'] for item in items])\n",
    "\n",
    "#     def calc_min_words_count(items):\n",
    "#         return np.min([articles_dic[item[0]]['words_count'] for item in items])\n",
    "\n",
    "#     def calc_max_words_count(items):\n",
    "#         return np.max([articles_dic[item[0]]['words_count'] for item in items])\n",
    "\n",
    "#     def calc_lag_between_created_at_ts_and_clicked_ts(items, articles_dic):\n",
    "#         item = items[-1]\n",
    "#         return (item[1] - articles_dic[item[0]]['created_at_ts']) / (1000 * 60 * 60 * 24)\n",
    "\n",
    "#     def calc_lag_between_two_click(items):\n",
    "#         if len(items) > 1:\n",
    "#             return (items[-1][1] - items[-2][1]) / (1000 * 60 * 60 * 24)\n",
    "#         else:\n",
    "#             return np.nan\n",
    "\n",
    "#     def calc_lag_between_two_articles(items, articles_dic):\n",
    "#         if len(items) > 1:\n",
    "#             return (articles_dic[items[-1][0]]['created_at_ts'] - articles_dic[items[-2][0]]['created_at_ts']) / (1000 * 60 * 60 * 24)\n",
    "#         else:\n",
    "#             return np.nan\n",
    "\n",
    "#     df_users = pd.DataFrame(list(test_users.keys()), columns=['user_id'])\n",
    "\n",
    "#     # 计算\n",
    "#     # 1. 用户看新闻的平均字数\n",
    "#     _data = []\n",
    "#     for user_id, ts_set in tqdm(test_users.items()):\n",
    "#         for last_clicked_timestamp in ts_set:\n",
    "#             _data.append((\n",
    "#                 user_id,\n",
    "#                 last_clicked_timestamp,\n",
    "#                 calc_avg_words_count(train_dataset[user_id][last_clicked_timestamp]),\n",
    "#                 calc_min_words_count(train_dataset[user_id][last_clicked_timestamp]),\n",
    "#                 calc_max_words_count(train_dataset[user_id][last_clicked_timestamp]),\n",
    "#                 calc_lag_between_created_at_ts_and_clicked_ts(train_dataset[user_id][last_clicked_timestamp], articles_dic),\n",
    "#                 calc_lag_between_two_click(train_dataset[user_id][last_clicked_timestamp]),\n",
    "#                 calc_lag_between_two_articles(train_dataset[user_id][last_clicked_timestamp], articles_dic),\n",
    "#                 ))\n",
    "\n",
    "#     df1 = pd.DataFrame(_data, columns=['user_id', 'last_clicked_timestamp', 'avg_words_count', 'min_words_count', 'max_words_count', 'lag_between_created_at_ts_and_clicked_ts', 'lag_between_two_click', 'lag_between_two_articles'])\n",
    "\n",
    "#     # 计算用户使用设备，环境等的众数\n",
    "#     columns = ['user_id','click_environment','click_deviceGroup','click_os','click_country','click_region','click_referrer_type']\n",
    "#     df2 = df_users.merge(raw_data.get_all_click_log())[columns].groupby('user_id').agg(lambda x: x.value_counts().index[0]).reset_index()\n",
    "\n",
    "#     return df1.merge(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面就是其他的一些代码了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 构建新的features，提供更多的预测的原料\n",
    "#         # 距离上次点击的时差\n",
    "#     ds['lag_period_last_article'] = ds['last_clicked_timestamp'] - ds['created_at_ts']\n",
    "#         # 和平均字数之间的差距\n",
    "#     ds['diff_words_last_article'] = ds['avg_words_count'] - ds['words_count']\n",
    "\n",
    "#     # 最终生成的数据集有这么多列：\n",
    "#         # user_id\tlast_clicked_timestamp\tarticle_id\tcategory_id\tcreated_at_ts\twords_count\tavg_words_count\tmin_words_count\tmax_words_count\tlag_between_created_at_ts_and_clicked_ts\tlag_between_two_click\tlag_between_two_articles\tclick_environment\tclick_deviceGroup\tclick_os\tclick_country\tclick_region\tclick_referrer_type\tlag_period_last_article\tdiff_words_last_article\n",
    "#     ds.to_csv(CACHE_FOLDER + '{}.csv'.format('train' if offline else 'test'), index=False)\n",
    "#     print('{}用的csv文件生成完毕({}秒, {}件)'.format('训练' if offline else '测试', '%.2f' % (time.time() - start_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用test()的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"<Start># Part_【1.2】make_test_data\");print()\n",
    "# test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test()函数定义的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test():\n",
    "#     df_train = pd.read_csv(CACHE_FOLDER + 'train.csv')\n",
    "\n",
    "#     # clf = lgb.LGBMRanker(random_state=777, n_estimators=1000)\n",
    "#     # 调低 n_estimators数量\n",
    "#     clf = lgb.LGBMRanker(random_state=777, n_estimators=100)\n",
    "\n",
    "#     users = df_train['user_id'].unique()\n",
    "#     train_users, _test_users = train_test_split(users, test_size=0.2, random_state=98)\n",
    "#     test_users, val_users = train_test_split(_test_users, test_size=0.5, random_state=38)\n",
    "#     df_new_train = df_train.merge(pd.DataFrame(train_users, columns=['user_id']))\n",
    "#     df_test = df_train.merge(pd.DataFrame(test_users, columns=['user_id']))\n",
    "#     df_val = df_train.merge(pd.DataFrame(val_users, columns=['user_id']))\n",
    "\n",
    "#     X_train, X_grp_train, y_train = prepare_dataset(df_new_train)\n",
    "#     X_test, X_grp_test, y_test = prepare_dataset(df_test)\n",
    "#     X_val, X_grp_val, _ = prepare_dataset(df_val)\n",
    "\n",
    "#     def handle_columns(X):\n",
    "#         return X.drop(columns=['user_id', 'article_id'])\n",
    "\n",
    "#     _X_train = handle_columns(X_train)\n",
    "\n",
    "#     clf.fit(_X_train, y_train, group=X_grp_train, eval_set=[(handle_columns(X_test), y_test)], eval_group=[X_grp_test], eval_at=[1, 2, 3, 4, 5], eval_metric=['ndcg', ], early_stopping_rounds=50, verbose=False)\n",
    "#     print('Best iteration: {}'.format(clf.best_iteration_))\n",
    "\n",
    "\n",
    "#     for X, X_grp, df, title in [(X_test, X_grp_test, df_test, 'Test Set'), (X_val, X_grp_val, df_val, 'Validation Set')]:\n",
    "#         print('[{}]'.format(title))\n",
    "#         y_pred = clf.predict(handle_columns(X), group=X_grp, num_iteration=clf.best_iteration_)\n",
    "#         recommend_dict = make_recommend_dict(X, y_pred)\n",
    "#         answers = dict(df.loc[df['answer'] == 1, ['user_id', 'article_id']].values)\n",
    "#         mrr, hit, details = calc_mrr_and_hit(recommend_dict, answers)\n",
    "#         print('MRR: {} / HIT: {}'.format(mrr, hit))\n",
    "#         print(' / '.join(['%.2f' % detail for detail in details]))\n",
    "\n",
    "#     for column, score in sorted(zip(_X_train.columns, clf.feature_importances_), key=lambda x: x[1], reverse=True):\n",
    "#         print('{}: {}'.format(column, score))\n",
    "#     print(\" Part_【1.2】make_test_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用这个函数的地方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     X_train, X_grp_train, y_train = prepare_dataset(df_new_train)\n",
    "#     X_test, X_grp_test, y_test = prepare_dataset(df_test)\n",
    "#     X_val, X_grp_val, _ = prepare_dataset(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数是如何定义的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_dataset(df):\n",
    "#     # 不知道意思是什么？\n",
    "#     agg_column = [column for column in df.columns if column != 'user_id'][0]\n",
    "#     # id 按照升序或者降序排列\n",
    "#     df.sort_values('user_id', inplace=True)\n",
    "#     # 不太懂\n",
    "#     grp_info = df.groupby('user_id', as_index=False).count()[agg_column].values\n",
    "#     # 不懂\n",
    "#     y = df['answer'] if 'answer' in df.columns else None\n",
    "#         # 不懂\n",
    "#         # 不知道输出的是什么，应该带着id,带着answer\n",
    "#     # s1把answer这一列删掉，删掉了成功再返回df, grp_info, y\n",
    "#     # df--你输入的dataframe；grp_info-不知道是什么；y代表你的预测值-打分\n",
    "#     return df.drop(columns=['answer']) if 'answer' in df.columns else df, grp_info, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make_recommend_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用这个函数的地方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommend_dict = make_recommend_dict(X, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数是如何定义的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recommend_dict(X_val, y_pred):\n",
    "    X_val['pred'] = y_pred\n",
    "    _t = X_val.groupby('user_id')\\\n",
    "        .apply(lambda x: list(x.sort_values('pred', ascending=False)['article_id'].head(5)))\\\n",
    "        .reset_index()\\\n",
    "        .rename(columns={0: 'item_list'})\n",
    "\n",
    "    recommend_dict = dict(zip(_t['user_id'], _t['item_list']))    \n",
    "    return recommend_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calc_mrr_and_hit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用这个代码的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mrr, hit, details = calc_mrr_and_hit(recommend_dict, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calc_mrr_and_hit()这个函数的定义的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_mrr_and_hit(recommend_dict, y, k=5):\n",
    "#     #assert len(recommend_dict) == len(y)\n",
    "#     sum_mrr = 0.0\n",
    "#     sum_hit = 0.0\n",
    "#     sum_hit_detail = np.repeat(0.0, 5)\n",
    "#     user_cnt = len(recommend_dict.keys())\n",
    "\n",
    "#     for user_id, recommend_items in recommend_dict.items():\n",
    "#         answer = y[user_id] if user_id in y else -1\n",
    "#         if (answer in recommend_items) and (recommend_items.index(answer) < k):\n",
    "#             sum_hit += 1\n",
    "#             sum_mrr += 1 / (recommend_items.index(answer) + 1)\n",
    "#             sum_hit_detail[recommend_items.index(answer)] += 1\n",
    "\n",
    "#     return (sum_mrr / user_cnt), (sum_hit / user_cnt), (sum_hit_detail / user_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用这句代码的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"<Start># Part_【2】make_test_data\");print()\n",
    "make_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make_test_data()函数的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_data():\n",
    "    handler(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "没什么好说的，这个东西就是用预处理数据并输出为csv文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用这句代码的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"<Start># Part_【3】run\");print()\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个run()函数的定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是读取处理过的数据、建模、预测、存储预测的东西-并打印"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run():\n",
    "#     df_train = pd.read_csv(CACHE_FOLDER + 'train.csv')\n",
    "#     df_test = pd.read_csv(CACHE_FOLDER + 'test.csv')\n",
    "\n",
    "#     # clf = lgb.LGBMRanker(random_state=777, n_estimators=1000)\n",
    "#     # !!!!!!!!!建议你将n_estimators变小一些，这样运算速度快!!!!!!!!!!!\n",
    "#     # 调低 n_estimators数量\n",
    "#     clf = lgb.LGBMRanker(random_state=777, n_estimators=100)\n",
    "\n",
    "#     users = df_train['user_id'].unique()\n",
    "#     train_users, eval_users = train_test_split(users, test_size=0.2, random_state=77)\n",
    "#     df_new_train = df_train.merge(pd.DataFrame(train_users, columns=['user_id']))\n",
    "#     df_eval = df_train.merge(pd.DataFrame(eval_users, columns=['user_id']))\n",
    "\n",
    "#     X_train, X_grp_train, y_train = prepare_dataset(df_new_train)\n",
    "#     X_eval, X_grp_eval, y_eval = prepare_dataset(df_eval)\n",
    "#     X_test, X_grp_test, _ = prepare_dataset(df_test)\n",
    "\n",
    "#     def handle_columns(X):\n",
    "#         return X.drop(columns=['user_id', 'article_id'])\n",
    "\n",
    "#     _X_train = handle_columns(X_train)\n",
    "\n",
    "#     clf.fit(_X_train, y_train, group=X_grp_train, eval_set=[(handle_columns(X_eval), y_eval)], eval_group=[X_grp_eval], eval_at=[1, 2, 3, 4, 5], eval_metric=['ndcg', ], early_stopping_rounds=50, verbose=False)\n",
    "#     print('Best iteration: {}'.format(clf.best_iteration_))\n",
    "#     y_pred = clf.predict(handle_columns(X_test), group=X_grp_test, num_iteration=clf.best_iteration_)\n",
    "    \n",
    "#     for column, score in sorted(zip(_X_train.columns, clf.feature_importances_), key=lambda x: x[1], reverse=True):\n",
    "#         print('{}: {}'.format(column, score))\n",
    "\n",
    "#     recommend_dict = make_recommend_dict(X_test, y_pred)\n",
    "\n",
    "#     create_submission(recommend_dict)\n",
    "#     print(\"<finished>Part_【3】run\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "473.181px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
